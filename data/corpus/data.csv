data
named entity real world object assigned name example person organization location detail check previous article fine tune bert ner ner summarized followslearn use spacy named entity recognition tasknatural language processing nlp lot use case spacy large contribution adoption nlp industry free open source python library designed handle large volume text data draw insight understanding text also suited production use used build information extraction ie natural language understanding nlu system pre process text deep learning spacy support tokenization part speech po tagging dependency parsing many others followssource spacy everything need know spacy usage documentationspacy pre trained model ton use case named entity recognition pre trained model recognize various type named entity text model statistical extremely dependent trained example work every kind entity might require model tuning depending use case hand let see example action first need import spacy load english model en_core_web_sm naming convention model language notation eg en english denoting language trained followed three component en_core_web_sm english language multi task convolutional neural network cnn trained ontonotes assigns context specific token vector po tag dependency parse named entity detail model check spacy model page pas text spacy object nlpto get detected entity need iterate ents property docthe entity detected spacy model followswe even visualize output using displacy followswe need set jupyter parameter true running jupyter notebook google colab let understand entire pipeline detailwhen use nlp spacy model object text spacy first tokenizes text produce doc object processed next stage pipeline pipeline used default model consists tagger stage parser stage entity recognizer ner stage pipeline component processed doc object passed next component stage source language processing pipeline spacy usage documentationto train spacy ner pipeline need follow step spacy model trained iterative process model prediction compared label ground truth order compute gradient loss gradient loss used update model weight backpropagation algorithm gradient indicate much weight value adjusted model prediction become similar closer provided label time source training pipeline model spacy usage documentationfor data preparation used ner annotation toolin interface need define entity type paste raw data need annotate select word required entity click mark completed done sample copy annotated data sample google colab spacy pre installed want run locally need install spacy package using following command notebookto convert data spacy format need create docbin object store data iterate data add example entity label docbin object save object spacy fileto create config file need go spacy training quickstart guide config page select ner component hardware based system availability also select optimize efficiency faster inference smaller model lower memory consumption higher accuracy potentially larger slower model impact choice architecture pretrained weight hyperparameters completed download config file clicking download button bottom right side source training pipeline model spacy usage documentationafter saved config file base_config cfg fill remaining field fill remaining field use following command use running notebook colab command create config cfg file use training ner pipeline config file contains several parameter needed training learning rate optimizer max_steps many others try change according needed configuration knowledge train custom pipeline run following commandas created validation set use training file validation however larger dataset create separate validation set validate model performance google colab en_core_web_sm already installed running locally install using following command notebookif command throw error due en_core_web_lg vector open config cfg file change vector parameter en_core_web_sm want use large model download using following commandonce pipeline trained store best model output directory load test example load model need use spacy load path let test text visualize result using displacy provide custom color dictionary entity want assign optionsisn cool trained custom named entity recognition pipeline using spacy v much ease try annotated dataset invent something new notebook found github hope liked article spacy named entity recognition share feedback suggestion comment connect linkedin medium shown article owned analytics vidhya used author discretion related
sql stand structured query language programming language interact query manage rdbms relational database management system sql skill highly preferred required used many organization large variety software application fresher experienced candidate planning upcoming sql interview need make sure prepared well sort conceptual theoretical coding sql question article consists real time scenario based sql coding interview question help test sql skill boost confidence note sql query used compatible oracle database version like g c c etc consider student table shown question q write query extract username character symbol email_id column answer extract position email id first using instr function pas position subtracting argument length substr function output q write query extract domain name like com au etc email_id column answer extract position dot character email id first using instr function pas position argument starting position substr function output q write query extract email service provider name like google yahoo outlook etc email_id column answer extract position email id first using instr function pas adding argument starting position substr function extract position dot character subtract earlier extracted position pas subtracting argument length substr function output q output following query b c answer bceil function return smallest integer number greater equal given number pas ceil return smallest integer value e floor function return largest integer number le equal given number pas floor return largest integer value e output q write query extract consonant present name answer first extract consonant input name extracted concatenate consonant character from_string argument remove consonant specifying corresponding character to_string argument pas narendra name query return vowel e output q write query extract vowel present name answer first extract consonant input name extracted concatenate consonant character from_string argument remove consonant specifying corresponding character to_string argument pas narendra name query return vowel e output refer emp table shown question q write query extract employee detail joined year answer use to_char extract year part hiredate column select employee hired using clause output q write query find hiked salary employee adding commission answer since commission column contains null value directly adding salary return null wherever commission null use nvl function determine hiked salary based whether commission null null comm expr null return sal comm expr comm null return sal expr output q write query find employee drawing salary manager answer self join emp table compare employee salary manager salary output q write query find subordinate reportees joined organization manager answer self join emp table compare employee hiredate manager hiredate output q write query find employee subordinate reportees e employee manager answer using simple subquery first find list distinct manager empnos select empno belong manager empnos output q write query find nd senior employee e joined organization second per hire date answer using correlated subquery get nd senior employee comparing inner query output clause output q write query find th maximum salary answer using correlated subquery get th maximum salary comparing inner query output clause output q write query find deviation average salary employee getting average salary note round average salary salary difference two digit answer first select employee getting average salary calculate deviation average salary employee output refer dept table along emp table question q write query find employee getting maximum salary department answer using simple subquery first get list maximum salary department using group operation select employee getting salary per list output q write query find department wise minimum salary maximum salary total salary average salary answer first inner join employee department table group deptno find minimum maximum total average salary department output q consider present table structure desired table structure customer table shown choose correct statement result desired table alter table customer rename custname name b alter table customer rename column custname name c alter table customer add email varchar alter table customer modify email varchar e alter table customer drop familysize f alter table customer drop column familysize answer b erename custname column name using modify email column datatype varchar varchar using drop familysize column using q consider following table schema data transaction table choose valid update statement update transaction set primestatus yes transactionid b update transaction set primestatus valid custname john c update transaction set transactionid null custname john update transaction set shoppingdate null transactionid answer dcheck constraint allows set predefined value n allowed primestatus column null constraint allow null value transactionid set null insert null value shoppingdate column output q output following sql statement avesumeb avsomc avsumd awesumeanswer cfor input string translate function replaces character specified from_string argument corresponding character to_string argument corresponding character to_string argument extra character present from_string argument removed input string output q consider emp insurance table shown get following output select emp left join insurance emp insurancetype insurance insurancetype b select emp join insurance emp insurancetype insurance insurancetype c select emp right join insurance emp insurancetype insurance insurancetype select emp full join insurance emp insurancetype insurance insurancetype answer c dright join return record right table along matched record left table since emp left table insurance type present insurance right table full outer join also return output note analyzing output focus record value rather sequence output medium shown article owned analytics vidhya used author discretion
although blockchain still infancy opportunity developer contribute exciting also many many business including supply chain automotive finance adopted blockchain without problem cryptocurrency namely bitcoin became popular first witnessed use case blockchain satoshi nakamoto established first bitcoin blockchain blockchain technology ability revolutionize virtual world handle data conduct commerce blockchain originally created platform support bitcoin demonstrating level flexibility security attracted curiosity many sector industry government pushing begin putting use mind stand reason blockchain developer good place start want professional lot possibility advancement vibrant new technology getting started however go becoming blockchain developer blockchain developer one go designing blockchain application article go everything need know becoming blockchain developer including ability need blockchain developer job come novel solution complex problem command control high integrity solution specialized product hardware company technical service line blockchain developer performs complex analysis design development testing computer software debugging software development operating system integration computer system selection task performed person finally work variety system must comfortable variety platform programming language naturally blockchain developer face difficulty example meeting need blockchain development project developer must work legacy infrastructure restriction also issue comprehending technological viability deploying decentralized cryptosystems procedure part regular infrastructure different type blockchain developersblockchain developer corethese people charge architecture design optimization guideline approve blockchain system designed developed optimised type blockchain developer consensus protocol example specifies way member blockchain resource agree share use make decision area well set blockchain functionality feature make sure work properly charge network security design implementation ensure network running create blockchain network connector service planning designing implementing want improve product feature function blockchain developerthese blockchain developer create maintain design according plan core developer develop application decentralised dapps implement smart contract way core developer intended ensure dapps function properly examining managing blockchain network integration service application engineer smart contractsthese blockchain developer evaluate create smart contract engage consumer buyer understand business flow security ensure smart contract free fault research smart contract test business process end end extremely popular cutting edge technology opportunity growth according pwc poll last year percent business interested hiring blockchain expert far larger number people use blockchain approximately company adopted technology many exploring near future pay excellent united state blockchain developer earn per year regular basis blockchain developer pay among highest industry according survey blockchain developer greater experience talent get paid digital security identity improved blockchain promise conventional non blockchain platform enterprise looking secure operation platform blockchain platform process digital id also assist blockchain developer company save money process operation responsibility issue becomes one prepare someone necessary ability take blockchain development challenge two different scenario work blockchain hopeful starting zero programming background well experience related field delving two distinct sort people want blockchain developer good idea familiarise mindset best suited blockchain developer unique problem blockchain development need unique mindset hear word hacker pronounced aloud usually positive light business worth salt want work hacker except ethical hacker another story hacker attitude hand lead creation skilled blockchain engineer due fact faced issue obstacle hacker prefer think outside box rather engage conventional thinking skilled blockchain developer also collaborate work well part team similar vein ideal blockchain developer understands seek help issue continue find solution result best candidate blockchain development work well people understands limitation come new solution challenge thorough understanding blockchain technologythis one go without saying grasp decentralized network work good blockchain developer blockchain technology exactly blockchain distributed database allows transparent secure tamper proof transaction operates leveraging peer peer network approve transaction eliminating need intermediary result great company trying cut expense improve productivity result thinking employing blockchain engineer double check set skill least one high level coding language masteryc golang c javascript solidity python ruby java among popular coding language blockchain development bitcoin well known cryptocurrency initially developed c language general purpose coding language blockchain engineer use range purpose exception solidity exclusive ethereum great blockchain developer excellent coder one language regardless situation solid cryptography security principle knowledgeblockchain technology safe since built encryption result solid understanding cryptography security principle required effective blockchain developer cryptography technique encrypting data highly difficult crack algorithm prohibits third party interfering information transferred party involved including transmitter recipient data situation cryptography put simply mean securing data without relying third party blockchain technology develops business need developer understand cryptography security principle result business ensure blockchain developer contemplating set skill thorough understanding distributed system peer peer networking required distributed system computer network allow user communicate sending receiving message put another way system decentralized meaning centralized control management blockchain network node peer central level control malfunction constraint hierarchy need comprehend principle blockchain developer knowledge smart contractssmart contract self executing contract party contract term written code electronic contract incorporate contract term condition two party contract implemented run without need third party intervene smart contract one powerful feature blockchain technology well one significant difference blockchain traditional database next year blockchain developer build smart contract high demand result keeping mind looking blockchain developer job crucial understanding data structure algorithmsa blockchain programmer able write algorithm following purpose necessary confirm balance validate handle newly added blockchain transaction consensus protocol put place verify validity digital signature create blockchain enabled application question blockchain developer successful fruitful professional life source concern let go complete path need take become blockchain developer worry mind start basic first foremost computer science information technology academic background bachelor master degree certain discipline might pursued formal education required become blockchain developer assist understanding fundamental laying framework studying becoming blockchain developer choose variety recommended training programme well degree programme obtain greater exposure specific technology furthermore practically every behemoth requires school credential prerequisite better chance landing desirable job gain technical proficiency necessary skill must master certain prerequisite technical skill entering blockchain development domain technological ability need become blockchain developer listed preceding section mostly consists programming ability database management data structure cryptography develop hacker mindset blockchain technology still early phase future unknown people imagine kind transformative application built blockchain coming year wonder blockchain developer hacker mentality building thing never built always meeting challenge always trying new solution order figure tackle obstacle confront daily basis make portfolio appeal portfolio feature best work done profession element portfolio demonstrate committed project benefitted customer past strong portfolio feature might help acquire freelance employment include case study testimonial data driven outcome photo chart work sample mock ups also highlight important talent assist progressing blockchain development profession join online community people share interest join online forum blockchain developer expert gain better knowledge technology kind group supply helpful knowledge also allowing establish strong professional network surprisingly network excellent resource learning employment possibility blockchain developer ready start career blockchain developer linkedin optymize upwork indeedso starting ground learn build blockchain site may enroll short course learn write java javascript python swift ground include coursera udemy skillshare udacity become blockchain developer need learn programming language thousand free online course available newcomer learn program various language furthermore advanced programming class language available various teaching platform take look following course pursue career blockchain developer beginner guide blockchain blockchain fundamentalsit take time become blockchain developer cliché go mastering something take thousand hour study practice however start path become blockchain developer right slow get disheartened take figure thing blockchain technology still infancy making difficult time consuming endeavor medium shown article owned analytics vidhya used author discretion
job interview scary fresher especially attending interview interdisciplinary role like data science machine learning tension doubt get yes end interview whether answer asked properly lead distraction preparation article sharing encountered question various subject data science ml ai previous interview experience although repetitive one may may get asked question often depend kind work done company organization example organization deal time series data getting time series question note matter curated question must toung tip attending data science interview without ado let u get subject confusion matrix describe role word understand type type ii error an real world machine learning confusion matrix useful way summarize machine learning model performance idea behind confusion matrix directly came perception great idea judge classification task accuracy imbalanced data biased skewed hence give wrong classification result confusion matrix thus provides better way check model performance confusion matrix matrix consisting key metric true positive false positive true negative false negative true positive tp correct prediction true eventsfalse positive fp incorrect prediction true event also called type error true negative tn correct prediction false eventsfalse negative fn incorrect prediction false event also called type ii error model well judged metric called sensitivity specificity accuracy calculated confusion matrix sensitivity measure correctly predicted proportion true positive formula tp tp fn specificity measure correctly predicted proportion true negative formula tn tn fp accuracy measure degree veracity classification simply proportion true prediction tp tn formula tp tn tp tn fp fn define precision recall f score formula an precision precision measure proportion true positive total number predicted positive help analyze quality prediction higher precision mean model generalized give almost correct result formula tp tp fp recall also called sensitivity true positive rate tpr probability detection recall ratio true positive total positive help measure true positive rate completeness prediction formula tp tp fn f score weighted harmonic mean precision recall f score take precision recall rate model performance formula x precision x recall precision recall would distinguish feature selection feature extraction an feature selection feature extraction two important technique scale high dimensional data solve curse dimensionality many feature may necessary hinders good modelling feature selection known fact increase input feature variable consume computational resource slow modelling lead overall failure feature selection stage data preprocessing feature essential predict target considered modelling irrelevant redundant feature removed stage robustness ml pipeline developed later depends hence plenty time effort given stage optimized feature set lead easier modelling fast computation robust machine learning pipeline easily scaled ready production feature extraction feature extraction process accelerates proper feature selection great extent feature extraction take task transforming feature existing data extract new feature feature extraction transformation done feature selection feature selection depends right top extracted feature data preprocessing stage feature extracted given data like n gram part speech tag text corner edge image transforming data feature selection done remove irrelevant redundant feature finally data mining exploring trend insight data modelling interpretation evaluation performed know autoregression moving average can not take non stationary data solve time series problem an generally used statistical tool work time series data autoregression technique us regression generally linear regression take input lag variable observation previous time step hence autoregression model output target variable future time step based linear combination lag variable let mathematically explain x b b x b x b b coefficient calculated optimization training datasetx predicted value timestamp x x input variable observation previous timestamps moving average simply calculated mean changing subset number sliding window mathematical term time series data use extremely helpful technique calculating average particular time variant feature certain period example stock market forecasting giant use moving average see stock trend market period help forecast stock headed next month also interchangeably called rolling mean let mean timestamp x find average x predict value timestamp sliding window hence take mean value predict rd value done slide next subset value moving average model instead taking value previous time step take lagged forecast error previous value white noise error predict ultimate forecast part ii can not take non stationary data solve time series problem never generally stationarity mean non fluctuation consistency machine learning model perform well data fitting stationary nature e without high fluctuation consistency input output parameter like mean median variance time series forecasting data highly fluctuating time period come seasonalities trend tend make data sharply inconsistent impossible model data highly fluctuating mean variance forecasting meet crisis extremely crucial check stationarity remove non stationarity data modelling explain hidden markov model called hidden an hidden markov model markov model hidden state hmms specially used solve problem using machine learning hidden target predicted based set observed feature attribute example mental health hidden state predicted set observed feature like sleep hour stress level work pressure physical health trauma abuse hidden markov model following probabilistic parameter using parameter hmm try answer question likelihood much likely observation happen based emission output probability get easily decide whether likely happen close far behind hidden markov model probabilistic inference hidden target drawn based maximum likelihood estimation markov process relies principle given present future always independent past hence name hidden markov model know autoencoders similar pca an autoencoders type artificial neural network architecture belonging wide diverse family anns uniqueness autoencoders feature learning algorithm produce output input unlike neural network output probability distribution class note autoencoders work mechanism data compression conventionally said latent space representation stacking multiple non linear transformation layer latent mean hidden classify dog cat image machine first place able point dog cat learning facial feature machine learning term structural similarity image really get see process work nope hence call latent space representation simple word hidden space representation task autoencoders compress input data latent space representation used extract feature similarity similar feature image lie coordinate close latent space extract feature similarity finally reconstruct using upsampling convolution operation output image compression go word dimensionality reduction compression heavily used deep learning reduce dimensionality image removing irrelevant detail focusing relevant one generally done encoding n dimensional data dimensional decoding produce original one although autoencoders resembles pca yet considered better pca case non linear feature space present pca handle linear transformation using non linear space lead loss relevant information autoencoders really useful reducing dimensionality high dimensional image data come expensive computational cost carrying computer vision task one numerous compression benefit autoencoders removing noise outputting clear data vanishing exploding gradient an model training stochastic gradient descent sgd computes gradient loss function respect weight neural network backpropagation calculation gradient update functional parameter weight minimize lost function chain rule differentiation product derivative depend parameter reside later network sometimes gradient respect weight previous layer network start becoming smaller smaller wrt time finally squash vanish hence vanishing gradient well product bunch number le going give u even smaller number subtracting small quantity weight would never really update weight pretty self explanatory exploding gradient upside vanishing gradient suppose gradient value large number say product bunch number greater going give u even greater number hence huge change weight epoch result large value therefore move away optimal value resulting explosion gradient explain tokenization stemming lemmatization nlp text preprocessing an tokenization basic text preprocessing stage large sentence piece text corpus broken smallest unit word often sentence represented token tokenization important text mining processing compute word frequency required modelling tokenizers generally two type word level tokenizer break text word level syntax tokenize word_tokenize return type return list word sentence level tokenizer break text sentence level syntax tokenize sent_tokenize return type return list sentence stemming rooting another text preprocessing technique word reduced base level root level example word stem root stem avoid different grammatical form word processed text though stemming work fine word may give weird nonsense output like give base strok word stroking strok even word lost correct data stroking process happens lot text fed stemmer lot information lost resulting model failure later lemmatization developed address loophole spotted stemming saw take care derivational affix lose original word return exactly dictionary base word called lemma lemmatization work logical approach keeping account part speech morphology text example feed united state stemmer get unite state whereas lemmatizer understands part speech proper noun return united state without altering original form briefly explain role tfidf algorithm solving nlp use case an tf idf short term frequency inverse document frequency information retrieval algorithm assigns score weight factor document present text corpus used carry nlp use case scoring directly help understanding important word sentence turn help eliminating extraneous irrelevant vocabulary corpus focus important highly scored word sentence tf idf computed using two kind frequency term frequency tf inverse document frequency idf term frequency tf frequency term word present document large text tf value may high word hence normalization tf done dividing frequency word w found document total number word document inverse document frequency idf idf assigns higher score simply weightage term word rarely found document rarer term higher idf weightage score note idf calculation degree uniqueness important document length increase word like therefore since etc increase large extent word really contribute machine learning part hence rare word document specific word occur le important scored higher idf example help search engine give really good result checking unique word entered search frequent term idf calculated taking log ratio total number document number document term thus tfidf nothing product tf idf tfidf tf idf central limit theorem accept reject null hypothesis explain valid point an central limit theorem simply state population data given kind distribution finite mean variance standard deviation sampling distribution sampling mean always normal gaussian distribution null hypothesis name suggests either nullified rejected let suppose think attacked corona virus thus covid positive assume call null hypothesis alternate hypothesis one want replace null hypothesis covid positive would want reject null hypothesis prove covid positive note either reject null hypothesis fail reject null hypothesis null hypothesis hypothesis tested failing reject null mean enough data support change innovation brought alternative reject null mean enough statistical evidence null hypothesis representative truth let u look visual representation given two tailed test graphically tail distribution show reject null hypothesis notice rejection region everything remains middle acceptance region rationale observed statistic far away depending significance level reject null otherwise accept calculate significance level probability rejecting null hypothesis true need calculate p value p value smallest level significance still reject null hypothesis given observed sample statistic important characteristic p value two type notable p value significance level alpha trick hypothesis testing follows compare contrast l regularization v l regularization an regularization common way controlling model complexity reducing overfitting flexible tunable manner l regularization l norm lasso regression work squashing model specific parameter towards simple word l regularization assigns value feature weight contribute much predictive ability machine learning model hence work like feature selection approach simply adding penalty term loss function generalized linear model regression example know squared loss regularization simply add penalty term penalize weight wj forcing irrelevant feature weight formula follows formula rocket science stuff penalty term added residual sum square r formula lambda hyperparameter regularization rate tune amount regularization applied model larger lambda coefficient shrunk toward zero model may underfit lasso l regularization add absolute value coefficient error term thus regularization term contributes optimal feature selection discarding many feature choose predictive modelling l regularization l norm ridge regression work similarly except penalty term instead taking absolute value coefficient take squared value formula l regularization follows differentiates l force coefficient weight towards never make equal regularization rate lambda used control rate penalization regularization element note higher lambda complexity decrease model underfits lower lambda complexity increase overfitting occurs hyperparameter tuning lambda need done carefully ensure robust performance model ensemble method superior individual model an ensemble method class machine learning algorithm combine several machine learning algorithm collectively called weak learner stack form ensemble learner strong learner combining prediction various weak learner one supervised model known type ensemble technique proven much superior individual model solving complex problem machine learning ensemble approach like bagging reduce variance boosting control bias stacking improve prediction great extent let examine ensemble learning yield better performance individual machine learning model bagging also known bootstrap aggregation bagging ensemble method extensively used work reducing variance noisy data help reducing sign overfitting example fitting many decision tree random different sample dataset averaging prediction made sample like divide conquer thus making model stabilized robust help reducing overfitting greatly dealing bias variance tradeoff boosting boosting work boosting strength working weakness boosting applies ensemble learning approach train sequence base model called weak learner learner compensates weakness precursor work idea united stand divided fall thus bagging strictly focus correcting prediction error stacking stacking basically stitch performance multiple classification regression model thus produce great prediction individual model stacking every single model take job learn best combination prediction done co worker stacking process consists base model followed meta model base model fit training data predict output altogether compiled meta model learns best combine prediction base model ensemble model ranked individual ml model due significance elbow method k mean clustering according opinion elbow method always give optimal number cluster an elbow method standardized approach calculating optimal number cluster unsupervised learning process k mean clustering algorithm step calculate optimal number cluster elbow method quite straightforward simple wcss within cluster sum squared error sound bit complex let break squared error point square distance point representation e predicted cluster center wcss score sum squared error point distance metric like euclidean distance manhattan distance used note number cluster increase wcss metric start fall wcss largest k according opinion elbow method bit naive approach often elbow exact steep enough choose optimal k may confusing many value seem found elbow region whereas one expected found plot really can not decide k could trial error still work often elbow region found mean optimal cluster problem analyzing model manager informed regression model suffering multicollinearity would check true without losing information still build better model an first thing first multicollinearity aesthetic way saying independent variable feature dataset highly correlated multicollinearity may serious issue disrupts independent nature independent variable make hard interpret coefficient reduces power model identify independent variable statistically significant self explanatory variable highly correlated change one variable would cause drastic change model result fluctuate significantly making non stationary model collapse need check feature dataset highly correlated multicollinearity checked standard practice plot correlation matrix right exploratory data analysis show pairwise correlation coefficient different independent variable spot multicollinearity bit time consuming task though variation inflation factor vif take le time sophisticated practice vif calculated independent variable using r squared value vif r² measure correlation one independent variable rest independent variable present dataset greater value multicollinearity research concludes vif mean serious multicollinearity le mean multicollinearity categorical continuous data hand use anova test check multicollinearity however continuous spearman rank correlation coefficient chi squared test enough removing independent variable multicollinearity bad idea may good predictor discarding lead loss important information better treat multicollinearity taking method practice performing l l norm ridge lasso regression weight penalizers reduces multicollinearity shrinking coefficient feature multicollinear data categorical variable one hot encoding scan often arise chance multicollinearity effective way remove setting drop_first true pd get_dummies function seen handle multicollinearity process help extensively build better model without losing information scalability mlops goal mlops experience idea scalability ml pipeline an machine learning training best model successful production ai ml pipeline must ensure scalability scalability mlops designing large scale model handle large amount data fed le time cost efficient way without consuming much memory cpu work smoothly million user throughout world good speed accuracy goal mlops straightforward accelerating scaling ml workload large scale ml model crucial managing processing large quantity data selecting optimized efficient machine learning algorithm deploying model production monitoring performance new data constructing necessary idea scaling ml workload follows amazon sagemaker one stop solution fast cost efficient scalable secure development training deploying ml pipeline scale one click training environment highly optimized machine learning algorithm built model tuning deployment without engineering effort life really le hectic cloud ml pipeline absolutely production ready whether batch prediction real time use flask develop machine learning application flask make stay away panic developing application source code help lot focus building best possible ml solution feature engineering data analysis crucial task really love simplicity flask creating data driven scalable web application line code tensorflow serving tensorflow serving flexible high performance serving system machine learning model designed production environment tensorflow serving make easy deploy new algorithm experiment keeping server architecture apis tensorflowhead see documentation detail ensure environment friendliness ml pipeline containerizing docker container make easy make code run smoothly across different environment operating system kind environmental dependency handled docker may save hour frustration help easier teamwork teammate using different virtual environment working ace interview must prepare basic statistical modelling product sense company interviewed kind analytics kind data business problem work practice implementing ml technique algorithm interview spend atleast hour day thoroughly studying kaggle top voted notebook searching topic simple keyword search pick problem statement various kaggle competition try solve remember perfectly fine view others take idea make sure code notebook brush sql python coding skill r rule made work everytime coefficient determination though determined enough revise review every project put resume matter well remember implementing revise code recalling challenge faced solved learnt journey throughout project necessary lot grueling question asked personal project lastly stay true try make answer thing know humbly say really know answer surely work help lot cheer reaching end article hope article help ace data science interview key takeaway article quick revision important data science stuff like central limit theorem l l regularization vanishing exploding gradient tip data science interview best like article data science interview question make sure follow linkedin discussion data science ml head github medium content data science medium shown article owned analytics vidhya used author discretion
memcached highly performant distributed caching system memory key value data store make type nosql database memcached used tech giant like facebook twitter instagram netflix previous article explained redis another memory key value data store used purpose caching article explain connect use memcached python memcached officially supported window however install setting window subsystem linux configuring alternatively run memcached container using docker covering article first step install docker window machine download docker desktop installation process fairly simple direct docker machine enter following command command prompt fetch redis image docker hub used build run container done third step start container using memcached image downloaded earlier click run button please refer picture reference congrats successfully started redis server machine connect use memcached python using python module called pymemcache installed running following command command prompt everything ready let get hand dirty dive programming part performing crud operation first need connect memcached server let go ahead connect memcached server successfully connected memcached server let start performing simple crud operation set key value pair use either set function add function accepts key value parameter please note key always either string data type byte get value key stored memcached server use get function accepts key name parameter response always byte hence response decoded string u manually fairly easy convert byte string simply decode byte utf format default data stored expire since memcached memory data store storage limited data might get flushed due insufficient memory set expiry key value pair using add function specifying time second expiration time carefully decided based type application building try access key second get response key would deleted database set multiple key value pair using set_multi function accepts multiple key value pair dictionary dataset also accepts expire parameter key value pair expire make space new data memcached server let u try increase value key ram make use incr function perform addition operation numerical value memcached similarly make use decr function perform subtraction operation numerical value memcached value apart numerical value key value pair would converted string byte storing would get string response original data structure might problem see stored list memcached server try fetch data get string response list make sure store data type well apart string make use serialization deserialization technique using module like json pickle using json explain serialization deserialization technique used store extract data data type let u first try store nested object memcached server see response get compared response get performing serialization deserialization using json try access name property would get error response string dictionary let u use json stringify data store memcached extract deserialize using json let try access area property present inside address property check getting error see get correct response caching system way speed application traditional sql database like mysql oracle new nosql database like mongodb store data disk operation performed disk generally slower compared operation performed memory ram since technology like redis memcached memory data store operation lightning fast downside technology ram limited can not used store information make application fast technology like redis memcached used alongside proper full fledged database like mysql mongodb etc let see memcached used alongside another database application let say set key value pair memcached server trying access assume key expired memcached server case try access data traditional database say mysql data persistent expire unlike memcached simple example show caching system designed work alongside full fledged database application article discussed covered following function discussed article necessary understanding performing basic crud operation memcached lot different function readily available caching mechanism present almost application must understand learn basic caching build production ready application memcached designed work caching server competitor redis built solve multiple use case redis single threaded memcached multi threaded make memcached perform better redis data size huge know redis read article article hope enjoyed reading article learned something new thanks reading happy learning medium shown article owned analytics vidhya used author discretion
field natural language processing e nlp lemmatization stemming text normalization technique technique used prepare word text document processing language english hindi consists several word often derived one another inflected language term used language contains derived word instance word historical derived word history hence derived word always common root form inflected word degree inflection varies lower higher depending language sum root form derived inflected word attained using stemming lemmatization package namely nltk stem used perform stemming via different class import porterstemmer nltk stem perform task instance ran run running derived one word e run therefore lemma three word run lemmatization used get valid word actual word returned wordnetlemmatizer library imported nltk stem look lemma word wordnet database note using wordnet lemmatizer wordnet corpus downloaded nltk downloader process reducing infected word stem instance figure stemming replace word history historical histori similarly word finally final stemming process removing last character given word obtain shorter form even form meaning need stemming nlp use case sentiment analysis spam classification restaurant review etc getting base word important know whether word positive negative stemming used get base word section help stemming paragraph using nltk used various use case sentiment analysis etc let get started note highly recommended use google colab run code import librariesimport library required stemming get inputthe paragraph taken input used stemming tokenization step stemming stemming tokenization done break text chunk case paragraph sentence easy computation seen output paragraph divided sentence based stemmingin code given one sentence taken time word tokenization applied e converting sentence word stopwords etc ignored stemming applied word finally stem word joined make sentence note stopwords word add value sentence python code output see stopwords removed sentence one word vision converted vision history histori stemming purpose lemmatization stemming overcomes drawback stemming stemming word may give may give meaningful representation histori lemmatization come picture give meaningful word lemmatization take time compared stemming find meaningful word representation stemming need get base word therefore take le time stemming application sentiment analysis lemmatization application chatbots human answering similar line stemming import library get input lemmatization import library get input tokenization step stemming output lemmatizationthe difference stemming lemmatization come step wordnetlemmatizer used instead porterstemmer rest step get outputoutput output noticed although word vision converted vision word history remained history unlike stemming thus retained meaning one thing note lot knowledge understanding structure language required lemmatization hence new language creation stemmer easier comparison lemmatization algorithm lemmatization stemming foundation derived inflected word hence difference lemma stem lemma actual word whereas stem may actual language word lemmatization us corpus attain lemma making slower stemming get proper lemma might define part speech whereas stemming step wise algorithm followed making faster point show stemming used speed important since lemmatizers scan corpus time consuming task choice lemmatizers stemmer also depends problem working medium shown article owned analytics vidhya used author discretion
creating collaborative data driven culture one important goal many modern organization data driven culture data used make decision every level organization data driven culture replacing gut feeling make decision fact however access data data processing tool remain restricted selected technical user upper management echelon average people skeptical see number willing play around data nate silver founder editor chief fivethirtyeight data driven culture can not exist without democratization data data democratization certainly mean unrestricted access organizational data aim data democratization data help employee make effective business decision available usable format fast enough access require technical data expert make sense survey result data scientist spend time average data scientist spend almost time data preparation good use time though despite spending time data preparation task data scientist find task least enjoyable part job observation coming one technically advanced community data user understandably people want insight data spend much time data preparation thus hindering propagation data driven culture overcome issue traditionally organization tasked centralized data engineering team create enterprise level data warehouse data lake analyst plug central data store get insight however model data delivery currently strain let paint situation imagine one need additional metric complete data analysis however metric requires additional data processed data warehouse get serviced centralized data engineering team employee need wait turn even minor change capable write themselvesthere three main issue centralized data engineering service interestingly solution scalability tailor made delivery model also come fashion industry readymade garment fashion company produce readymade garment various standard fit size ready product however customer still need minor customization always get done fraction cost time compared bespoke solution scratch building similar analogy data engineering team achieve scale centrally managing heavy technical aspect establishing technology process people practice enabling minor customizations done business user self service model let u see component centrally delivered managed service component plan decentralize business logic component previous data pipeline domain specific business logic component decentralized handle technical user closer domain data analyst data scientist organization struggle propagate data driven culture due significant challenge one way promote data driven culture promoting hybrid data delivery model hybrid delivery model promote data democratization providing self service business logic centrally managing heavy lifting data preparation modern data stack driving force behind concept hybrid data delivery model look technical detail achieve leveraging right tool architecture upcoming article era big data starting point end pearl zhu digital master serieswith modern data stack inflection point end user enablement necessity wishlist truly data driven culture please reach linkedin conversation enjoyed reading article forget follow feedback welcome many thanks reference medium shown article owned analytics vidhya used author discretion
image asked business case challenge interview machine learning engineer data scientist comparable position typical become nervous top firm like faang like integrate business case problem screening process day approach followed leading company like uber twitter case study open minded technical specific company interviewing business case problem data science interview basic term case study real world project may work company type case study question asked determined position interviewing interview given around minute absorb problem description walk thought potential solution can not measure can not improve peter drucker getting right metric case study important difficult due different reason like fresher knowledge industry note business case problem single correct answer simple solution mainly case study categorized three type product based case study prediction based case study machine learning business case study pretend interviewing position twitter engagement team twitter news feed know provides content user based interest assistance news feed ranking algorithm task part engagement team assigned task evaluate algorithm success image given problem never begin firing technique instead start clarifying case study make absolutely sure interviewer page question tend confusing indefinite asking question help get answer extra information show curious case study open minded avoid word like correct approach would might multiple solution problem statement begin phrase like going problem like double check understood problem keeping track approach much easier take note absolutely ok pause consider plan proceeding solution demonstrate understood problem concentrate actually problem example study problem focused twitter news feed user engagement result algorithm success evaluated first step identify relevant indicator ass share comment click rate ctr strategy choose address problem next step question like might included also thinking measurement always follow trend may rising others may falling circumstance may pick metric based approach perspective whether want focus business user experience consider ad business standpoint ctr powerful statistic user share content powerful metric user experience comment reaction example text processing comment take step back ask interviewer question proceed completed key phase process sure cover following clarifying brainstorming strategy conclusionmake certain approach case study complete also discus result method regard problem statement response structured manner conclude case study end end solution sensible strategy know target audience panic say irrelevant point instead tell interviewer stuck let know thinking could proceed approach case interviewer might give clue sure leading towards solution might work lead dead end hesitate mention might lead situation instead concentrate would correct continue strategy problem encountered real life project expected identify problem continue appropriate approach could add new approach problem avoiding problem stuck let interviewer know sure approach think loud get track always better bluffing practice ask case study record camera observe correct help gaining confidence practicing peer practice get better making note practice study help refer future resource found useful practice case study http www interviewquery com blog data science case study interviewvoila follow four step successful case study response clarify plan strategy conclusion word one memorize approach case study caper c clarify assume p plan e execute r review make interview participatory possible taking note benefit structuring response mention pitfall let know pro con chosen approach make sure know know company product practice peer mock interview worry first effort mess take feedback seriously hope liked article data science interview case study please share feedback comment section image http mockinterview co index php four sample case study data scientist analytics position image http www searchenginejournal com increase google search visibility twitter medium shown article owned analytics vidhya used author discretion
photo sebastian herrmann unsplashas know previous post started mailing list promote blog last post series designed way answer question causing folk subscribe blog matching opt source checked already find time take second look subscriber history data question validity today need step sherlock home shoe time put detective cap solve mystery like good analyst validating subscriber_feed_history table built together give history subscribed mailing list noticed inconsistency table especially compared actual truth remember source table mailing_list feed complete list user opted mailing list particular day well turn subscriber_feed reliable thought use subscriber_feed_history table see many user opted particular day match email_sent_history table derived log email sends align odd assume user opted particular day sent email day turn case welcome world data two table working today small caveat send email daily email_sent_history table date one email sent stop reading ahead want take time think problem reading approach let think approach problem step first need determine user subscribed day email sent since email sent daily compare email sent list day determine two thing email sent email look opted day email opted day sent email finally figured final step would alter subscriber_feed_history table accurately account inconsistency get email opted particular day leverage subscriber_feed_history directly need expiry date check duration user opted purpose built subscriber_feed_history_view add expiry date table see user subscriber_feed_history_view look something like figured next step compare email_sent_history subscriber_feed_history_view see going way full join user opted sent email get side coin remember joining user mailing_list_status time email sent sql get u see user null value opt_in_users field sent email opted vice versa null email_sent_users figured way get inconsistency next step think fix subscriber_feed_history let start thinking user sent email opted well logical thing opt right like date two scenario invalid opt user unsubscribe event recorded email sent missed opt user look seems never opted sent email first case may consider opt captured invalid delete sure implement fix work case opt email day sent email fix subscriber_feed_history would entail adding opt record user subscriber_feed_history hand user appear opted sent email must opted right make assumption revisit future let think two scenario invalid opt user_id seems unsubscribed part send missed opt user_id never seems unsubscribed part email real question unsubscribe given data say best estimate would last email sent date example user_id part email let say must unsubscribed two date lack better knowledge assume day last send apply fix subscriber_feed_history add opt event new date follows accomplished today started identifying inconsistency email subscriber history table built list user subscribed mailing list align list user sent promotional email combat start comparing two data source subscriber email sent history identify user subscribed sent email vice versa design approach case fix original table accounting inconsistency voila tweaking subscriber_feed_history aligned real truth key takeaway photo linkedin sale solution left ben white right unsplashlet know approach found helpful share find twitter abhishek talk data medium shown article owned analytics vidhya used author discretion
teenager listed item paper buy shop called data case today world get lot data medium store vast amount data impossible write data book suppose use book store data time consuming easy retrieve data even alter data information may lost due bad weather condition misfortune need system store data retrieve update easily explained introduction data may class price item etc data collection fact e word number picture database repository data provides functionality adding modifying querying data dbms database management system software manage database one type dbms rdbms also called relational database management system set software tool control data access organization storage especially bank healthcare use rdbms example rdbms mysql oracle database ibm db db express cache data model resemble database data model used relational model allows data independence data stored table provides logical data independence physical data independence physical storage independence entity relationship data model alternative relational data model entity relationship data model entity object exist independently entity entity attribute data element describe entity example book entity edition year price title etc attribute entity becomes table attribute column figure entity relationship data model mapping entity table basic foundation designing database begin erd entity relationship diagram map erd table entity becomes table attribute column separate entity attribute example building block relational model set relation relation made part relational schema relational instance relational schema specifies following name relation schema name type attribute relation instance table made row column column attribute row tuples degree number attribute cardinality number tuplesthere mechanism establish data integrity two relation relational database called referencing primary key relational table uniquely identifies row table foreign key set column referring primary key another table table containing primary key related least one foreign key dependent table consists one foreign key business data must often adhere certain restriction called constraint constraint help implement business rule following six constraint entity integrity constraint primary key unique value identifies row tuple attribute participating primary key accept null value example book_id primary key must unique must null value table constraint referential integrity defines relationship table ensures remain valid validity data enforced using combination primary key foreign key example semantic integrity constraint related correctness data example column contain garbage value like table domain constraint allows valid value given attribute example book_name must character book_price must numeric table null constraint attribute value allowed null example book_name must null table check constraint rule set rule help check inserted updated data value table based condition example book_price must negative value table sql statement namely ddl dml ddl data definition language statement used create modify delete database object table example create alter truncate drop create statement used create table define column note basic data type use sql char varchar number date clob syntax create table column_name_ datatype optional_parameters column_ name_ datatype column name_n datatype code create table product prodid number primary key prodname varchar null qty number check qty description varchar output ii alter statement used altering table including adding dropping column modifying data type syntax alter tableadd column_name datatype code alter table product add model_no varchar null output iii truncate statement used deleting data table table syntax truncate table code truncate table test iv drop statement used deleting table syntax drop table code drop table test dml data manipulation language statement used modify data table example dml create read update delete insert statement populates table data data manipulation language statement used read modify data syntax insert value code insert product prodid prodname qty_ available description value laptop dell output ii select statement clause data manipulation language used read modify data syntax select fromcode select emp output iii update statement data manipulation language statement used read modify data syntax update set columnname value wherecode update emp set sal sal comm empno output iv delete statement data manipulation language used read modify data syntax delete wherecode select test output code delete test empno output group function grouping group function group function sum max min avg count code select max sal min sal sum sal emp output give name group function code select max sal high min sal low sum sal total emp output grouping process computing aggregate aggregating based one column grouping generated using group clause syntax select group function group code select deptno sum sal emp group deptno output clause order used filter grouped data used filter non grouped data used group clause used group clause order used end statement syntax select column name group order code select job max sal emp deptno group job max sal order output article learnedi hope found article useful several programming language like python r scala sa use sql syntax data analytics us sql analysis even use sql machine learning algorithm hope see next article medium shown article owned analytics vidhya used author discretion
